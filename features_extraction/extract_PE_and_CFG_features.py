#!/usr/bin/env python3

import argparse
import os
import math
import statistics
import hashlib
from collections import Counter
from tqdm import tqdm

import pefile
import networkx as nx
import numpy as np
import pandas as pd
import json
import r2pipe

def compute_entropy(data):
    """Calculate the Shannon entropy (in bits) of a given byte sequence."""
    if not data:
        return 0.0
    occurrences = [0] * 256
    for byte in data:
        occurrences[byte] += 1
    total = len(data)
    probabilities = [float(occ) / total for occ in occurrences if occ > 0]
    return -sum(p * math.log2(p) for p in probabilities)

def extract_pe_info(file_path):
    """
    Extract PE metadata and characteristics using pefile.
    Returns a dict of metadata or None if there's an error.
    """
    try:
        pe = pefile.PE(file_path)

        # Compute MD5 hash
        with open(file_path, "rb") as f:
            md5_hash = hashlib.md5(f.read()).hexdigest()

        optional_header = pe.OPTIONAL_HEADER
        file_header = pe.FILE_HEADER

        # Keep all original PE features
        pe_info = {
            "Filename": os.path.basename(file_path),
            "MD5": md5_hash,
            "Machine": file_header.Machine,
            "SizeOfOptionalHeader": file_header.SizeOfOptionalHeader,
            "Characteristics": file_header.Characteristics,
            "MajorLinkerVersion": optional_header.MajorLinkerVersion,
            "MinorLinkerVersion": optional_header.MinorLinkerVersion,
            "SizeOfCode": optional_header.SizeOfCode,
            "SizeOfInitializedData": optional_header.SizeOfInitializedData,
            "SizeOfUninitializedData": optional_header.SizeOfUninitializedData,
            "AddressOfEntryPoint": optional_header.AddressOfEntryPoint,
            "BaseOfCode": optional_header.BaseOfCode,
            "BaseOfData": getattr(optional_header, "BaseOfData", 0),  # PE32 only
            "ImageBase": optional_header.ImageBase,
            "SectionAlignment": optional_header.SectionAlignment,
            "FileAlignment": optional_header.FileAlignment,
            "MajorOperatingSystemVersion": optional_header.MajorOperatingSystemVersion,
            "MinorOperatingSystemVersion": optional_header.MinorOperatingSystemVersion,
            "MajorImageVersion": optional_header.MajorImageVersion,
            "MinorImageVersion": optional_header.MinorImageVersion,
            "MajorSubsystemVersion": optional_header.MajorSubsystemVersion,
            "MinorSubsystemVersion": optional_header.MinorSubsystemVersion,
            "SizeOfImage": optional_header.SizeOfImage,
            "SizeOfHeaders": optional_header.SizeOfHeaders,
            "CheckSum": optional_header.CheckSum,
            "Subsystem": optional_header.Subsystem,
            "DllCharacteristics": optional_header.DllCharacteristics,
            "SizeOfStackReserve": optional_header.SizeOfStackReserve,
            "SizeOfStackCommit": optional_header.SizeOfStackCommit,
            "SizeOfHeapReserve": optional_header.SizeOfHeapReserve,
            "SizeOfHeapCommit": optional_header.SizeOfHeapCommit,
            "LoaderFlags": optional_header.LoaderFlags,
            "NumberOfRvaAndSizes": optional_header.NumberOfRvaAndSizes,
        }

        # Sections
        sections = pe.sections
        entropy_values = [compute_entropy(section.get_data()) for section in sections]
        raw_sizes = [section.SizeOfRawData for section in sections]
        virtual_sizes = [section.Misc_VirtualSize for section in sections]

        pe_info.update({
            "SectionsNb": len(sections),
            "SectionsMeanEntropy": statistics.mean(entropy_values) if entropy_values else 0,
            "SectionsMinEntropy": min(entropy_values, default=0),
            "SectionsMaxEntropy": max(entropy_values, default=0),
            "SectionsMeanRawsize": statistics.mean(raw_sizes) if raw_sizes else 0,
            "SectionsMinRawsize": min(raw_sizes, default=0),
            "SectionMaxRawsize": max(raw_sizes, default=0),
            "SectionsMeanVirtualsize": statistics.mean(virtual_sizes) if virtual_sizes else 0,
            "SectionsMinVirtualsize": min(virtual_sizes, default=0),
            "SectionMaxVirtualsize": max(virtual_sizes, default=0),
        })

        # Import Table
        try:
            import_dlls = len(pe.DIRECTORY_ENTRY_IMPORT)
            import_entries = sum(len(entry.imports) for entry in pe.DIRECTORY_ENTRY_IMPORT)
            ordinal_imports = sum(1 for entry in pe.DIRECTORY_ENTRY_IMPORT for imp in entry.imports if imp.ordinal)
        except AttributeError:
            import_dlls, import_entries, ordinal_imports = 0, 0, 0

        pe_info.update({
            "ImportsNbDLL": import_dlls,
            "ImportsNb": import_entries,
            "ImportsNbOrdinal": ordinal_imports,
        })

        # Export Table
        try:
            export_nb = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
        except AttributeError:
            export_nb = 0
        pe_info["ExportNb"] = export_nb

        return pe_info

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

def generate_cfg(filepath):
    """Build and return a Control Flow Graph (CFG) using Radare2."""
    # Open file with r2pipe with necessary options to suppress warnings
    r2 = r2pipe.open(filepath, flags=["-e", "bin.relocs.apply=true", "-e", "log.quiet=true"])
    r2.cmd("aaa")  # analyze all

    # Get function list
    functions_json = r2.cmd("aflj")
    if not functions_json.strip():
        r2.quit()
        return nx.DiGraph()

    try:
        functions = json.loads(functions_json)
    except json.JSONDecodeError:
        r2.quit()
        return nx.DiGraph()

    cfg_graph = nx.DiGraph()

    # Process each function
    for f in functions:
        func_name = f.get("name")
        if not func_name:
            continue

        graph_json = r2.cmd(f"agj @ {func_name}")
        if not graph_json.strip():
            continue

        try:
            graph_data = json.loads(graph_json)
        except json.JSONDecodeError:
            continue

        # Process blocks and edges
        for g in graph_data:
            if "blocks" in g:
                for block in g["blocks"]:
                    blk_addr = block["offset"]
                    cfg_graph.add_node(blk_addr, size=block.get("size", 0))

                    if "ops" in block:
                        for op in block["ops"]:
                            jump = op.get("jump")
                            fail = op.get("fail")
                            if jump is not None:
                                cfg_graph.add_edge(blk_addr, jump)
                            if fail is not None:
                                cfg_graph.add_edge(blk_addr, fail)

    r2.quit()
    return cfg_graph

def extract_cfg_features(nx_graph):
    """Extract numerical features from a Control Flow Graph."""
    features = {}
    
    num_nodes = nx_graph.number_of_nodes()
    num_edges = nx_graph.number_of_edges()
    features['CFG_num_nodes'] = num_nodes
    features['CFG_num_edges'] = num_edges
    features['CFG_density'] = nx.density(nx_graph)
    
    if num_nodes > 0:
        out_degrees = [deg for _, deg in nx_graph.out_degree()]
        in_degrees = [deg for _, deg in nx_graph.in_degree()]

        features['CFG_avg_out_degree'] = float(np.mean(out_degrees))
        features['CFG_max_out_degree'] = max(out_degrees)
        features['CFG_max_in_degree'] = max(in_degrees)
    else:
        features['CFG_avg_out_degree'] = 0.0
        features['CFG_max_out_degree'] = 0
        features['CFG_max_in_degree'] = 0
    
    num_components = nx.number_weakly_connected_components(nx_graph)
    features['CFG_connected_components'] = num_components
    features['CFG_cyclomatic_complexity'] = num_edges - num_nodes + num_components
    
    try:
        features['CFG_avg_path_length'] = nx.average_shortest_path_length(nx_graph.to_undirected())
    except nx.NetworkXError:
        features['CFG_avg_path_length'] = -1
    
    try:
        longest_path = nx.dag_longest_path(nx_graph)
        features['CFG_longest_path_length'] = len(longest_path) - 1
    except nx.NetworkXUnfeasible:
        features['CFG_longest_path_length'] = -1
    
    if num_nodes > 0:
        features['CFG_num_entry_points'] = sum(1 for n in nx_graph.nodes() if nx_graph.in_degree(n) == 0)
        features['CFG_num_exit_points'] = sum(1 for n in nx_graph.nodes() if nx_graph.out_degree(n) == 0)
    else:
        features['CFG_num_entry_points'] = 0
        features['CFG_num_exit_points'] = 0

    return features

def compute_outdegree_distribution(nx_graph, max_degree_bin=5):
    """Compute out-degree histogram."""
    out_degs = [deg for _, deg in nx_graph.out_degree()]
    deg_count = Counter(out_degs)

    features = {}
    for d in range(max_degree_bin + 1):
        features[f"CFG_outdeg_{d}"] = deg_count[d]
    features["CFG_outdeg_greater"] = sum(
        count for degree, count in deg_count.items() if degree > max_degree_bin
    )
    return features

def build_feature_vector(nx_graph, max_degree_bin=5):
    """Combine CFG-based features + outdegree distribution into one dict."""
    base_feats = extract_cfg_features(nx_graph)              
    outdeg_dist = compute_outdegree_distribution(nx_graph, max_degree_bin)
    return {**base_feats, **outdeg_dist}

def process_file(file_path, max_degree_bin=5, is_malware=True):
    """Process a single EXE file, extracting PE info and CFG features."""
    try:
        # Print the current file being analyzed
        print(f"Analyzing: {file_path}")
        
        # Extract PE info
        pe_info = extract_pe_info(file_path)
        if not pe_info:
            return None
        
        # Build CFG
        r2_cfg = generate_cfg(file_path)

        # Extract CFG features
        cfg_feats = build_feature_vector(r2_cfg, max_degree_bin)

        # Combine everything into one row
        row = {**pe_info, **cfg_feats}
        row["malware"] = 1 if is_malware else 0
        return row
    except Exception as e:
        print(f"Failed to process {os.path.basename(file_path)}: {e}")
        return None

def process_folder(folder_path, output_csv, max_degree_bin=5, is_malware=True):
    """Process all files in a folder with progress tracking."""
    
    # Find all files in the folder
    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    
    if not files:
        print("No files found in the specified folder.")
        return
    
    print(f"Found {len(files)} files to analyze.")
    rows = []
    
    # Process files with progress bar
    for filename in tqdm(files, desc="Analyzing PE files", unit="file"):
        file_path = os.path.join(folder_path, filename)
        row = process_file(file_path, max_degree_bin, is_malware)
        if row:
            rows.append(row)
    
    if not rows:
        print("No valid PE files or no data extracted.")
        return

    # Save results to CSV
    df = pd.DataFrame(rows)
    df.to_csv(output_csv, index=False)
    print(f"\nDone! Results saved to {output_csv}")
    print(f"Successfully processed {len(rows)}/{len(files)} files ({len(rows)/len(files)*100:.1f}%).")

def main():
    parser = argparse.ArgumentParser(
        description="Extract PE metadata and CFG features from executables."
    )

    parser.add_argument("-p", "--path", help="Alternative way to specify the folder path")
    parser.add_argument("-o", "--output", default="pe_cfg_features.csv", help="Output CSV file name")
    parser.add_argument("--max_degree_bin", type=int, default=5, help="Max bin for out-degree histogram")
    
    # Add mutually exclusive group for good/bad options
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--good", action="store_true", help="Mark samples as benign (malware=0)")
    group.add_argument("--bad", action="store_true", help="Mark samples as malicious (malware=1)")
    
    args = parser.parse_args()

    folder_path = args.path
    if not folder_path:
        parser.error("You must specify a folder path")
        return
        
    if not os.path.isdir(folder_path):
        print(f"Error: The folder '{folder_path}' does not exist.")
        return

    # Determine if files should be marked as malware or not during the analysis
    # If pointing to a folder containing malware samples, use malware=1  :)
    is_malware = not args.good  # Default to malware=1 unless --good is specified
    
    process_folder(folder_path, args.output, args.max_degree_bin, is_malware)

if __name__ == "__main__":
    main()
