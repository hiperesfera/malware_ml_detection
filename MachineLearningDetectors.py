from pandas import read_csv
import matplotlib 
import sklearn 
import timeit

url="Kaggle-data.csv"
dataset = read_csv(url)
#In the dataset you want to predict the legitimate class. 1 means benign-ware and 0 means malware. The rest of the 
#data is the information that you will use for the prediction.

#Some things that yu don't need are the ID, md5, Machine
#Clean the dataset and remove these parts
dataClean = dataset.drop(['ID','md5','Machine','Unnamed: 57'],axis=1)
print(dataClean)

from sklearn.model_selection import train_test_split

#For the example we reduce the dataset from 216351 to 10000 instances, you can change this in your 
#final experiments and add more data

dataCleanReduce = dataClean.sample(n=1000)
#print(dataCleanReduce)
array= dataCleanReduce.values
# X are the columns that we are going to use for training the classifier
X = array[:,0:53]
# Y is the column that we will try to predict (legitimate) this says whether a piece of software is malware
#or not 
Y = array[:,53]

#We have some data for training and some data for the final evaluation
#The classifier will learn from the training data and will be evaluated with the test data
#This will reduce overfitting

#Normally when you split the dataset you have 80% for training and 20% for test
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=1, shuffle=True)
print(Y_train)

#We are going to create our machine learning models with the classifiers

#Classifiers
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
#This is a list of models and each of them is going to be a classifier
models=[]
#models.append(("Tree",DecisionTreeClassifier()))
#models.append(("KNN",KNeighborsClassifier()))
#models.append(("LDiscrimination",LinearDiscriminantAnalysis()))
#models.append(("NB",GaussianNB()))
#models.append(("SVM",SVC(gamma="auto")))
#models.append(("LRegression",LogisticRegression(solver="liblinear",multi_class="ovr")))
models.append(("RandomForest",RandomForestClassifier()))
#models.append(("GradientBoosting",GradientBoostingClassifier()))
#models.append(("AdaBoost",AdaBoostClassifier()))
models.append(("XGBoost",XGBClassifier()))
#models.append(("NNet",MLPClassifier(random_state=1, max_iter=300)))
#models.append(("OneRule",StackingClassifier()))
#This list will accumulate the results
results=[]
names = []

for name, model in models:
        #Normally you divide the training data in 10 blocks (or n blocks) and you use 9 for training and one
        #for testing, then you change the blocks 10 times and you choose form the 10 models that you have 
        #created the best one. This reduces overfitting
        start=timeit.default_timer()
        cv_fold= StratifiedKFold(n_splits=10,random_state=1,shuffle=True)
        cv_results= cross_val_score(model, X_train,Y_train,cv=cv_fold, scoring="accuracy")
        stop=timeit.default_timer()
        results.append(cv_results)
        names.append(name)
        print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))
        print("Time: ",stop-start)

#This is to create a plot comparing the accuracy of the different classifiers

from matplotlib import pyplot

pyplot.boxplot(results, labels=names)
pyplot.title("Malware Detector Comparison")
pyplot.xticks(rotation='vertical')
pyplot.show()
